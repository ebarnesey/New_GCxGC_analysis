---
title: "GCxGC Processing: Blob Tables to Timelines. SeaScape Aerosol Example"
author: "Emily Barnes"
date: "September 10, 2020"
output: github_document
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, message = FALSE, warning=FALSE}
library(dplyr)
library(tidyverse)

library(scales)

library(akima)
#library(asbio)
#library(tmap)         # raster + vector layers
#library(raster)       # Main raster library
#library(tidyverse)    # our old friend
#library(sf)           # to work with simple features data
#library(mapview)
library(openxlsx)
library(class)
library(RANN)
library("survival")
library("Hmisc")


#install.packages("survival")

library(lattice)
#install.packages("latticeExtra")


library(survival)
library(Hmisc)
#install.packages("installr")
#library(installr)
```
## Importing and Tidying GC Image Data
Background: GC image pre-processing
- All images must be RI configured and searched against 1) a custom library of internal standard compounds 2) a custom library of field blank compounds 3) one or more custom libraries of template compounds to be traced over campaign.  Libraries need not be compiled into one and can remain separated by source file. 
- Note: manually checking for appropriate internal standard assignments strongly advised

Required files:
- Blob table summary.  See example for formatting; match exactly to avoid bugs. Note: if GCxGC file name convention differs from that in example blob table summary, adjust the date and file parsing code accordingly
- Folder with all blob tables to be parsed
- all blob tables from pre-processed GC image files in csv format
```{r timeline_import, message=FALSE, warning=FALSE}
bt_sum_bloom3 <- read_csv("Blob_table_summary_CAICESample.csv") %>% 
  filter(Bloom == 3)

bt_sum_bloom <- read_csv("Blob_table_summary_CAICESample.csv") %>% 
  filter(Category == "Sample")

bt_sum_bloom3 <- bt_sum_bloom3 %>% mutate(r_date =    mdy_hm(SS_startdate)) %>% 
  mutate(run_date = as.Date(gsub("GCxGC_","",File_num), "%Y%m%d"))

make_file_name <- function(date_string) {
  file_start <- "CAICE_blobtables/"
  file_end <- ".h5_img01_Blob_Table.csv"
  full_name <- paste(file_start,date_string,file_end, sep = "")
  return(full_name)
  
}

read_bt_files <- function(fi_name_short) {
  
  fi_name <- make_file_name(fi_name_short)
  temp_bt <<- read_csv(file = fi_name)
  temp_bt <- temp_bt %>% mutate(File_num = fi_name_short)
  temp_bt <<- temp_bt
  return(temp_bt)
}

make_massive_table <- function(summary_table){
 
  M <- read_bt_files(as.character(summary_table$File_num[1]))

  for(i in 2:length(summary_table$File_num)){
    t <- read_bt_files(as.character(summary_table$File_num[i]))
    Mnew <- rbind(M, t)
    M <- Mnew
    print(i)
  }
  M_t <<- M
}

make_massive_table(bt_sum_bloom3)


M_t_full3 <- M_t %>% 
  left_join(bt_sum_bloom3, by = "File_num") %>% 
  mutate(Punch_t_norm_vol = Volume/Punch_num_sample_time_norm)

names(M_t_full3) <- make.names(names(M_t_full3),unique = TRUE) 

make_massive_table(bt_sum_bloom)


M_t_full <- M_t %>% 
  left_join(bt_sum_bloom, by = "File_num") %>% 
  mutate(Punch_t_norm_vol = Volume/Punch_num_sample_time_norm)

names(M_t_full) <- make.names(names(M_t_full),unique = TRUE) 


```

condensing to unique plots

```{r}
Match_factor_floor <- 750
Reverse_match_factor_floor <- 100
LRI_diff_floor <- 6




fb_match_factor_floor <- 600
fb_reverse_match_factor_floor <- 100
IS_lib_name <- "caice_ssi"
lib_remove_1 <- "caice_ssa_0805_0612"
lib_remove_2 <- "caice_ssa_0803_1728"
lib_remove_3 <- "caice_ssa_0803_oil_1728"
lib_remove_4 <- "amzi0503_b"
#FB_lib_name <- "amz_fb_trimmed_esrem"

rt2_floor <- .2 # note: check on this, but for purposes of indexing retention times in 2d this is important

# creating a column of the differences in linear retention indecies so that poor matches can be screened out
M_t_LRI <- M_t_full3 %>% mutate(LRI_diff = abs(Library.RI-LRI.I)) %>% 
  mutate(LRI_diff = replace_na(LRI_diff, -999))

# getting rid of things that should be removed
remove_match <- M_t_LRI %>% 
  filter(Library.Name == lib_remove_1 | Library.Name == lib_remove_2 | Library.Name == lib_remove_3 | Library.Name == lib_remove_4)

# identifying the internal standard
IS_goodmatch <- M_t_LRI %>% 
  filter(Library.Name == IS_lib_name | Library.Name == "-") %>% 
  filter(LRI_diff < LRI_diff_floor) %>% 
  filter(Library.Match.Factor > Match_factor_floor| Description== "match")  

# FB_goodmatch <- M_t_LRI %>% 
#   filter(Library.Name == FB_lib_name) %>% 
#   filter(LRI_diff < LRI_diff_floor) %>% 
#   filter(Library.Match.Factor > fb_match_factor_floor)
# 
# FB_okmatch <- M_t_LRI %>% 
#   filter(Library.Name == FB_lib_name) %>% 
#   filter(Library.Match.Factor> fb_match_factor_floor-100) %>% 
#   anti_join(FB_goodmatch)
  
IS_okmatch <- M_t_LRI %>% 
  filter(Library.Name == IS_lib_name | Library.Name == "-") %>%
  filter(Library.Match.Factor > Match_factor_floor-100)  %>% 
  anti_join(IS_goodmatch)

small_poorlymatched <- M_t_LRI %>% 
  filter(Library.Match.Factor < Match_factor_floor) %>% 
  filter(Volume < 200)

M_t_all_analytes <- M_t_LRI %>% 
  #anti_join(FB_goodmatch) %>% 
  anti_join(IS_goodmatch) %>% 
  #anti_join(FB_okmatch) %>% 
  anti_join(IS_okmatch) %>% 
  anti_join(remove_match) %>% 
  anti_join(small_poorlymatched) %>% 
  filter(LRI.I > 1300) 

## Getting rid of things that existed and were well matched in the tap water blank


tap_water_blank <- read_csv("CAICE_tapwater_ssasearched.csv")

names(tap_water_blank) <- make.names(names(tap_water_blank),unique = TRUE) 

tap_water_blank_matches <- tap_water_blank %>% 
  filter(Library.Match.Factor > 750) %>% 
  filter(Library.Name != "caice_ssi") %>% 
  dplyr::select(Compound.Name) %>% 
  mutate(in_background = 1)

M_t_all_analytes <- M_t_all_analytes %>% 
  left_join(tap_water_blank_matches) %>% 
  filter(is.na(in_background))

  
M_t_analyte_unique <- M_t_all_analytes %>% 
  filter(Library.Name != IS_lib_name) %>% 
  filter(LRI_diff < LRI_diff_floor) %>% 
  filter(Library.Match.Factor > Match_factor_floor) %>% 
  #filter(Library.Reverse.Match.Factor > Reverse_match_factor_floor) %>% 
  arrange(desc(Volume)) %>% 
  arrange(Compound.Name) %>% 
  arrange(r_date) %>% 
  distinct(Compound.Name, r_date, .keep_all = TRUE)






```
Important to note: the blob names are messed up for the template from 0805- still listed as from 0803


Tracing Library Performance
```{r}
# tracking the mass of all of the analytes before bad matches are screened out for later analysis of library performance
M_t_all_analytes_volcount <- M_t_all_analytes %>% 
  group_by(File_num) %>% 
  summarise(rawvol = sum(Volume))

# tracking the number of all analutes in
M_t_all_analytes_ncount <- M_t_all_analytes %>% 
  count(File_num) %>% 
  rename(total_num_analyte = n)

M_t_match_analytes_volcount <- M_t_analyte_unique %>% 
  group_by(File_num) %>% 
  summarise(rawvol_match = sum(Volume))

M_t_match_analytes_ncount <- M_t_analyte_unique %>% 
  count(File_num) %>% 
  rename(total_num_analyte_match = n)

M_t_analyte_summary <- M_t_all_analytes_volcount %>% 
  left_join(M_t_all_analytes_ncount) %>% 
  left_join(M_t_match_analytes_volcount) %>% 
  left_join(M_t_match_analytes_ncount) %>% 
  mutate(perct_nfound = (total_num_analyte_match/total_num_analyte)*100) %>% 
  mutate(perct_volfound = (rawvol_match/rawvol)*100)

compound_pop <- M_t_analyte_unique %>% 
  count(Compound.Name) %>% 
  rename(comp.n = n) %>% 
  mutate(pct_of_samp = (comp.n/max(comp.n)*100))

#table(compound_pop$comp.n)
#hist(compound_pop$comp.n)

# 
# #note: potential issue with screening out bad fb matches eek
# M_t_analyte_count <- M_t_full %>%
#   filter(Library.Name != "amzi0503_b") %>%
#   filter(Library.Name != "fb_amz_compiled_t") %>%
#   count(File_num) %>%
#   rename(total_num_analyte = n)

M_t_analyte_count <- M_t_all_analytes %>% 
  count(File_num) %>%
  rename(total_num_analyte = n)
# 
# # counting the number of unique analytes in each image
num_analyte_unique <- M_t_analyte_unique %>%
  count(File_num) %>%
  rename(unique.analyte = n)

# counting the number based percent of compounds being traced in all 
perct_comps_traced <- num_analyte_unique %>% 
   left_join(M_t_analyte_count) %>% 
   mutate(pct_found = unique.analyte/total_num_analyte)
# 
#perct_comps_traced %>%
#  ggplot(aes(y = pct_found))+
#  geom_boxplot()

summary(perct_comps_traced$pct_found)


M_t_analyte_unique <- M_t_analyte_unique %>% left_join(num_analyte_unique)

compound_pop <- M_t_analyte_unique %>% 
  count(Compound.Name) %>% 
  rename(comp.n = n) %>% 
  mutate(pct_of_samp = (comp.n/max(comp.n)*100))

table(compound_pop$comp.n)

hist(compound_pop$pct_of_samp, 
     main = paste("Histogram of", nrow(compound_pop), "Traced Compounds\nOver Seascape Bloom 3"),
     xlab = "Percent Occurence Above Detection Limits",
     col = "grey")

SeaScape_sample_cumcum <- M_t_analyte_unique %>% 
  group_by(File_num) %>% 
  summarise(File_vol = sum(Volume))


M_t_analyte_unique <- M_t_analyte_unique %>% 
  left_join(compound_pop) %>% 
  left_join(SeaScape_sample_cumcum) %>% 
  mutate(Volume_Fraction = Volume/File_vol)


M_t_analyte_unique %>% 
  mutate(char_comp_n = as.integer(round(pct_of_samp, 0))) %>% 
  group_by(char_comp_n) %>% 
  summarise(avg_vol_byn = mean(Volume)) %>% 
  ggplot(aes(x = char_comp_n, y = avg_vol_byn)) +
  geom_col()+
  labs(title = "Average Blob Volume by Library Match Frequency")+
  xlab("Percent of Samples with Positive Library Match")+
  ylab("Average Blob Volume")

```

```{r fig.width=2, fig.height=3, warning = FALSE}


M_t_analyte_summary %>% 
  ggplot(aes(y = perct_nfound))+
  geom_boxplot(fill = "pink") + 
  theme_bw()+
  ylab("Percent of Analytes Assigned to \nLibrary Match")

M_t_analyte_summary %>% 
  ggplot(aes(y = perct_volfound))+
  geom_boxplot(fill = "pink") + 
  theme_bw()+
  ylab("Percent of Analyte Volume Assigned to \nLibrary Match")

summary(M_t_analyte_summary$perct_volfound)

#weighted average of distances
#leave one out cross validation

#kriging- from geography
#GAM to build 2d surface 
# try predict with GAM or loess surface, 
# link function for GAM
# could use a moving window 
```

Tracking appearance of new compounds
```{r}

unique_tracing <- M_t_analyte_unique %>% 
  arrange(r_date)

unique_tag <- 
  unique_tracing %>% 
  filter(File_num == bt_sum_bloom3$File_num[1]) %>% 
  mutate(is_unique_prior = NA) %>% 
  dplyr::select(Compound.Name, File_num, is_unique_prior)


for(i in 2:nrow(bt_sum_bloom3)){
  
  
  
  previous_file_i <- bt_sum_bloom3$File_num[i-1]
  file_i <- bt_sum_bloom3$File_num[i]
  
  prev_compounds <- unique_tracing %>% 
    filter(File_num == previous_file_i) %>% 
    mutate(is_unique_prior = FALSE) %>% 
    dplyr::select(Compound.Name, is_unique_prior)
  
  compounds_i <- unique_tracing %>% 
    filter(File_num== file_i)
  
  
  unique_tag_t <- compounds_i %>% 
    left_join(prev_compounds, by = "Compound.Name") %>% 
    dplyr::select(Compound.Name, File_num, is_unique_prior)
  
  unique_tag <- rbind(unique_tag, unique_tag_t)
    
  
}

unique_tracing_c <- unique_tracing %>% 
  left_join(unique_tag, by = c("Compound.Name", "File_num")) %>% 
  mutate(unique_log = is.na(is_unique_prior)) %>% 
  mutate(unique_vol = unique_log * Volume/(Punch_num_sample_time_norm)) 


avg_unique = sum(unique_tracing_c$unique_vol)/sum(unique_tracing_c$Volume)

```

more unique tracing

```{r}
unique_tracing_c %>% 
  #filter(IOP == 2) %>% 
  mutate(ones = 1) %>% 
  ggplot(aes(fill = unique_log, y = ones, x = r_date))+
  geom_bar(position = "stack", stat = "identity")+
  ylab("Analyte Count")




unique_tracing_c %>% 
  #filter(IOP == 2) %>% 
  mutate(ones = 1) %>% 
  ggplot(aes(fill = unique_log, y = Punch_t_norm_vol, x = r_date))+
  geom_bar(position = "stack", stat = "identity")+
  ylab("Analyte Raw Volume")+
  xlab("Dry Season GoAmazon Date")+
  scale_fill_manual(name = "Unique Compared\n to Previous Sample", values=c("blue", "green"))+
  theme(axis.text.x = element_text(size=14), axis.text.y = element_text(size=14), axis.text=element_text(size=12),
        axis.title=element_text(size=14))


  
  
```
Adding in cumsum and pct found to analyte unique for raw timelines
```{r pressure, importing chemical info}

compiled_comp_info <- read_csv("Compiled_Compoundinfo_Blobtable_SS.csv")

compiled_comp_info.findable <- compiled_comp_info %>% 
  filter(Library.Match.Factor_NISTmain> 800) %>% 
  mutate(Identifiable = TRUE) %>% 
  dplyr::select(Compound.Name, Identifiable)

#id.tags <- data.frame("Identifiable"= c(TRUE, NA), ID)

compiled_comp_info <- compiled_comp_info %>% 
  left_join(compiled_comp_info.findable) %>% 
  mutate(tic = 1)

compiled_comp_info %>% 
  ggplot(aes(x = "", y = tic, fill = Identifiable)) + 
  geom_col()+
  coord_polar("y", start = 0)+
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  labs(fill = "",
       x = NULL,
       y = NULL) + 
  ggtitle("735 Novel and Identifiable Submicron SSA Compounds- SeaScape")

```


```{r}
SeaScape_cumsum_raw <- M_t_analyte_unique %>% 
  group_by(Compound.Name) %>% 
  summarise(raw_cumsum = sum(Punch_t_norm_vol)) %>% 
  arrange(desc(raw_cumsum))





M_t_analyte_unique_info <- M_t_analyte_unique %>% 
  left_join(SeaScape_cumsum_raw, by = "Compound.Name") %>% 
  left_join(compiled_comp_info, by = "Compound.Name") %>% 
  left_join(compiled_comp_info.findable, by = "Compound.Name")

for(i in 1:nrow(compiled_comp_info.findable)){
  findable_c <- compiled_comp_info$Compound.Name[i]
  
  f1 <- M_t_analyte_unique_info %>% 
    filter(Compound.Name == findable_c) %>% 
    ggplot(aes(x = r_date, y = Punch_t_norm_vol, color = Compound.Name_NISTmain))+
    geom_line()
  
  print(f1)
  
  
  
  
}


for(i in 1:100){
  big_c <- SeaScape_cumsum_raw$Compound.Name[i]
  
  f2 <- M_t_analyte_unique_info %>% 
    filter(Compound.Name == big_c) %>% 
    ggplot(aes(x = r_date, y = Volume_Fraction, color = Compound.Name_NISTmain))+
    geom_line()+
    geom_point(aes(x = r_date, y = Volume_Fraction, shape = T_O_D, fill = Library.Match.Factor_NISTmain))
  
  print(f2)
  
  
  
  
}

for(i in 1:100){
  big_c <- SeaScape_cumsum_raw$Compound.Name[i]
  
  f3 <- M_t_analyte_unique_info %>% 
    filter(Compound.Name == big_c) %>% 
    ggplot(aes(x = r_date, y = Punch_t_norm_vol, color = Compound.Name_NISTmain))+
    geom_line()+
    geom_point(aes(x = r_date, y = Punch_t_norm_vol, shape = T_O_D, fill = Library.Match.Factor_NISTmain))
  
  print(f3)
  
  
}



```


Internal Standard Mapping

```{r IS_mapping}
# note: unlike sample analytes, internal standard compounds are frequently split vertically into multiple blobs because of the high volume.  In order to account for this, I am summing all blobs together that meet the high match criteria and are very close in the first dimension.
IS_unique <- IS_goodmatch %>% 
  group_by(Compound.Name, File_num) %>% 
  summarise(Volume_tot = sum(Volume)) %>% 
  filter(Compound.Name != "glucose-13C6")

IS_unique_c <- IS_unique %>% 
  mutate(tic = 1) %>% 
  group_by(File_num) %>% 
  summarise(ticsum = sum(tic))

c<- data.frame(table(IS_unique$Compound.Name))

IS_positions <- IS_goodmatch %>% 
  group_by(Compound.Name, File_num) %>% 
  #summarise(LRI_avg = mean(LRI.I), RI2 = min(Retention.II..sec.-rt2_floor))
  summarise(LRI_avg = mean(LRI.I), RI2 = min(Retention.II..sec. - rt2_floor))

IS_avg_vols_toadd <- IS_unique %>% 
  group_by(Compound.Name) %>% 
  summarise(IS_avgvol = mean(Volume_tot), IS_medvol = median(Volume_tot)) %>% 
  left_join(IS_unique) %>% 
  mutate(IS_mean_norm = Volume_tot / IS_avgvol) %>%
  mutate(IS_med_norm = Volume_tot / IS_medvol) %>% 
  dplyr::select(Compound.Name, File_num, IS_mean_norm)

IS_unique_pos <- IS_unique %>% 
  left_join(IS_positions) %>% 
  left_join(IS_avg_vols_toadd)



IS_test1 <- IS_unique_pos %>% 
  filter(File_num == "GCxGC_20200803_1001")

IS_1_vol <- IS_test1$Volume_tot
RI1 <- IS_test1$LRI_avg
RI2 <- IS_test1$RI2

# IS_1_pos <- IS_test1 %>% 
#   ungroup() %>% 
#   dplyr::select("LRI_avg", "RI2")
IS_positions_202008061332 <- read.csv("GCxGC_20200806_1332.h5_img01_IS_selected_positions_Blob_Table.csv")
IS_rt2 <- IS_positions_202008061332 %>% 
  dplyr::select(Compound.Name, Retention.II..sec.)


IS_positions_avg <- IS_positions %>% 
  group_by(Compound.Name) %>% 
  summarise(LRI_avg_all = mean(LRI_avg, na.rm = TRUE)) %>% 
  left_join(IS_rt2)


write.csv(IS_positions_avg, "IS_positions_avg.csv")


IS_avg_vols <- IS_unique %>% 
  group_by(Compound.Name) %>% 
  summarise(IS_avgvol = mean(Volume_tot), IS_medvol = median(Volume_tot)) %>% 
  left_join(IS_unique) %>% 
  mutate(IS_mean_norm = Volume_tot / IS_avgvol) %>%
  mutate(IS_med_norm = Volume_tot / IS_medvol) %>% 
  left_join(IOP12_btsum)

IS_cat <- read.csv("IS_withcat.csv")
IS_cat <- IS_cat %>% 
  dplyr::select(-X)

IS_avg_vols <- IS_avg_vols %>% 
  left_join(IS_cat)
  

IS_just_one_IS <- IS_avg_vols %>% 
  filter(Compound.Name == "dC24") %>% 
  dplyr::select(File_num, dalk_norm=IS_mean_norm) 

IS_stability <- IS_avg_vols %>% 
  left_join(IS_just_one_IS) %>% 
  left_join(IS_positions_avg)

IS_stability %>% 
  ggplot(aes(x = dalk_norm, y = IS_mean_norm, color = F_group_cat))+
  geom_point()+
  ylim(0, 2) +
  xlim(0,2)
  


IS_avg_vols %>% ggplot(aes(x = r_date, y = IS_med_norm, color = Compound.Name)) +
  geom_point()
  

IS_avg_vols %>% ggplot(aes(x = r_date, y = Volume_tot, color = Compound.Name)) +
  geom_point()
```

## Including Plots

You can also embed plots, for example:



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
